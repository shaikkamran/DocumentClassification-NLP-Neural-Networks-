
# coding: utf-8

# In[1]:


import pandas as pd
import gensim
import nltk
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense,Flatten
from keras.layers.recurrent import LSTM
from keras.layers.embeddings import Embedding
from keras.models import load_model
import numpy as np
import gensim
from gensim.models import Word2Vec
from gensim import utils
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter, defaultdict
from sklearn.cross_validation import train_test_split


# In[2]:


df=pd.read_excel('awesummly_classification_dataset.xlsx')
tf=pd.read_excel('New_test_data_Textclassification.xlsx')


# In[4]:


df = df.append(tf, ignore_index=True)


# In[5]:


X_train=df['Text']
Y_train=df['Category']
Y_train.unique()


# In[6]:


from nltk.corpus import wordnet as wn


# In[7]:


def generate_redundancies(arr):
    k=0
    x_arr=[]
    for i in arr:
        c=[]
        for j in i:
            
        
            for synset in wn.synsets(j):
                for lemma in synset.lemmas():
                    if lemma.name() not in c:
                        c.append(lemma.name())
        k+=1
        x_arr.append(c)
    return x_arr    


# In[8]:


newarr=[]
for i in range(len(X_train)):
    pos=nltk.pos_tag(nltk.word_tokenize(X_train[i]))
    c=[]
    for word,x in pos:
        if x in ['NN','NNP','NNS','NNPS','VB','VBP','VBD','VBG','VBN','VBZ']:
            c.append(word)
    newarr.append(c)        
      


# In[9]:


x_arr=generate_redundancies(newarr)


# In[10]:


#cateogorising
cat=df['Category'].unique()


# In[11]:


cat=list(cat)


# In[12]:


test=[]
for val in Y_train:
    if val in cat:
        test.append(cat.index(val))


# In[43]:


cat


# In[14]:


test=to_categorical(test)


# In[15]:


len(test)


# In[16]:


len(x_arr)


# In[17]:


import re, nltk
from nltk.stem.porter import PorterStemmer

stemmer = PorterStemmer()
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

def tokenize(text):
    # remove non letters
    text = re.sub("[^a-zA-Z]", " ", text)
    # tokenize
    tokens = nltk.word_tokenize(text)
    # stem
    stems = stem_tokens(tokens, stemmer)
    return stems

class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(next(iter(word2vec.items())))

    def fit(self, X):
        tfidf = TfidfVectorizer(analyzer=lambda x: x,tokenizer = tokenize,lowercase = True,stop_words = 'english')
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of 
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

        return self

    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])


# In[18]:


new_model = gensim.models.KeyedVectors.load_word2vec_format('/home/ravil/Sentiment analysis/googles pretrained/GoogleNews-vectors-negative300.bin', binary=True, limit=500000)


# In[19]:


w2v = dict(zip(new_model.wv.index2word, new_model.wv.syn0))
tfidfv=TfidfEmbeddingVectorizer(w2v)


# In[20]:


tfidfv.fit(x_arr)


# In[21]:


tfidfv=tfidfv.transform(x_arr)


# In[22]:


x_arr=np.array(tfidfv)


# In[23]:


x_arr.shape


# In[24]:


test[1].shape


# In[25]:


X_train, X_test, y_train, y_test = train_test_split(x_arr,test, 
                                                    test_size=0.2, 
                                                    random_state=0)


# In[26]:


X_train.shape


# In[27]:



# set parameters:
max_features = 5000
maxlen = 400
batch_size = 64
embedding_dims = 50
filters = 500
kernel_size = 5
hidden_dims = 500
epochs = 50


# In[476]:


from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.layers import Dense, Dropout, Activation
# define the model
model = Sequential()
model.add(Dense(512,input_shape=(300,)))
model.add(Dropout(0.5))
model.add(Activation('relu'))

# We add a vanilla hidden layer:
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Activation('relu'))

model.add(Dense(64))
model.add(Dropout(0.2))
model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(8))
model.add(Activation('softmax'))

# compile the model
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])


# summarize the model
print(model.summary())


# In[477]:


X_train=X_train.reshape(187, 300)


# In[478]:


X_test=X_test.reshape(47, 300)


# In[479]:


history=model.fit(X_train,y_train ,validation_data=(X_test, y_test) ,epochs=40,verbose=1,shuffle=True)


# In[480]:


import matplotlib.pyplot as plt
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()


# In[491]:


# evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=1)
print('Accuracy: %f' % (accuracy*100))


# In[28]:


from keras.models import load_model
model=load_model("fchr.h5")


# In[248]:


#X_pred=[""" past year has been tumultuous for fixed-income investors. Demonetisation, which brought a gush of liquidity into the banking system, resulted in a sharp fall in market interest rates. Bond prices went through the roof, with investors in longer- and medium-duration debt mutual funds reaping windfall gains. Oil prices collapsed to a low of $43 a barrel in the first half of 2017. Fall in oil prices in an economy which imports most of its requirement, positively impacts current account balance, fiscal deficit and the rupee. Consumer price inflation consequently reached its nadir, at 1.54 per cent in June 2017. It appeared that the bond market party, a direct beneficiary of low inflation, was unstoppable. The money markets continued to witness liquidity surplus keeping yields low. Two events caught the markets by surprise, one geo-political and the other local. A young, impatient Crown Prince of the world's largest oil exporter Saudi Arabia, arrested some of the most powerful members of his own royal clan in an overnight move, as part of an anti-corruption drive. Oil prices which were already in a recovery mode, touched two-year highs. Market talk of a more disciplined OPEC cartel in controlling oil production and a developing cozy relationship between erstwhile enemies - the US shale oil producers and OPEC - further added to the bull market frenzy in oil prices. The local event to shake-up the bond market was the announcement of the PSU bank recapitalisation scheme. This added to the existing angst on account of fiscal deficit slippage. RBI too was on an open-market operations (OMO) sale spree to drain liquidity from the markets. The Indian bond markets reacted swiftly to these global and local events. 10-year G Sec rates shot up by about 60 basis points, with longterm debt funds' NAVs taking a hit and medium/short term funds seeing poor returns. Though the yields have corrected to some extent on account of RBI's withdrawal of OMO sales and the Moody's upgrade, they stay elevated. The fixed-income investor is today in a difficult situation. Taxes eat away nearly a third of the earnings from bank fixed deposits, which yield 6.5 per cent pretax. Though perplexing, the government continues to pay a much higher 8 per cent on its taxable bonds, available on tap. While the return is attractive for the investor in the lower-tax brackets, the after-tax yield for investors paying a marginal rate of tax at 30.9 per cent is a meagre 5.5 per cent. Tax-free bond issuance from AAA-rated government entities, such as IRFC and NHAI, has stopped. Erstwhile fixed-income investing advocates have thrown in the towel and are advising investors to stay away from long-duration funds, in anticipation of a further rise in bond yields and a corresponding fall in their prices. The yield-to-maturity of relatively conservative short-term debt mutual funds is barely 7 per cent. The returns investors get in hand is further reduced by the expense ratios charged to the funds. With cost inflation indexation falling to 3 per cent, projected yield after tax and post expense is a paltry 6 per cent. The conversation on fixed-income investing inevitably turns to credit opportunity funds, with attractive YTM's of about 8.5 per cent in today's yield-starved scenario, but carrying higher credit risk. The adage that comes to mind beware of Greeks bearing gifts is an allusion to the devious gift of the Trojan horse from Greece to Troy, during the Trojan War. Investors have been stung by spectacular defaults in certain debt schemes which offered high yield. A US-based fund house exited its Indian mutual fund business after one such episode in which its debt fund investors lost money. Investors should take serious note of statutory warnings from mutual funds that their schemes are subject to risk. Unlike a bank deposit, mutual fund investments do not come with any guarantee or protection from the fund sponsor. With relatively conservative debt funds offering meager returns, the funds with higher yields and greater credit risk imploding once in a while, and an uncertain outlook for bond prices, it is no wonder that even die-hard fixed-income investors are increasingly turning to equity mutual funds despite the vagaries of the markets."""]
#X_pred=["""Entertainment is a form of activity that holds the attention and interest of an audience, or gives pleasure and delight."""]
X_pred=["""In an exemplary punishment to erring cops, Patna DIG Rajesh Kumar has ordered of cease 70 Station House Officers' (SHOs) salaries in Patna district for the month of November. The order was issued after an angry DIG found that none of the police officers made any breakthrough or a single arrest in their jurisdiction despite several cases of crime reported in last one week. Rajesh Kumar had recently held a meeting to review the law and order situation in Patna and found that there have been multiple cases of murder, loot, dacoity and kidnapping that have happened in rural and urban parts of the city. Kumar had warned the senior cops to take actions and make arrests of the accused as most of the crime cases were high profile and the police headquarters was monitoring the investigations in these cases on a day to day basis. On Thursday, when Patna DIG held another round of meeting on law and order, he found that despite giving strict warning, the police officers made no effort to arrest any of the accused. An angry Rajesh Kumar said that the police officers had manifested indiscipline and negligence for their work and ordered to stop the salaries of 70 SHOs. DIG has also sought an explanation from 10 DSPs in Patna as to why the police officers in their jurisdiction made no arrest in last one week in cases of murder, loot and kidnapping."""]


# In[249]:


#X_pred=["""In an exemplary chemistry punishment to erring cops, Patna DIG Rajesh Kumar has ordered of cease 70 Station House Officers' (SHOs) salaries in Patna district for the month of November. The order was issued after an angry DIG found that none of the police officers made any breakthrough or a single arrest in their jurisdiction despite several cases of crime reported in last one week. Rajesh Kumar had recently held a meeting to review the law and order situation in Patna and found that there have been multiple cases of murder, loot, dacoity and kidnapping that have happened in rural and urban parts of the city. Kumar had warned the senior cops to take actions and make arrests of the accused as most of the crime cases were high profile and the police headquarters was monitoring the investigations in these cases on a day to day basis. On Thursday, when Patna DIG held another round of meeting on law and order, he found that despite giving strict warning, the police officers made no effort to arrest any of the accused. An angry Rajesh Kumar said that the police officers had manifested indiscipline and negligence for their work and ordered to stop the salaries of 70 SHOs. DIG has also sought an explanation from 10 DSPs in Patna as to why the police officers in their jurisdiction made no arrest in last one week in cases of murder, loot and kidnapping."""]


# In[250]:


newarr=[]
for i in range(len(X_pred)):
    pos=nltk.pos_tag(nltk.word_tokenize(X_pred[i]))
    c=[]
    for word,x in pos:
        if x in ['NN','NNP','NNS','NNPS','VB','VBP','VBD','VBG','VBN','VBZ']:
            c.append(word)
    newarr.append(c)        
      


# In[251]:


predictdata=newarr


# In[252]:


tfidfv=TfidfEmbeddingVectorizer(w2v)
tfidfv.fit(newarr)
tfidfv=tfidfv.transform(newarr)
newarr=np.array(tfidfv)


# In[253]:


newarr=newarr.reshape(1,300)


# In[254]:


predictions=((model.predict(newarr)))
predictions=predictions.flatten()


# In[255]:


res = defaultdict(list)
for i,val in enumerate(predictions):
    res[cat[i]].append(val)
    #print(cat[i],"       ",predictions[0][i])

newres=res


# In[256]:


res=sorted(res, key=lambda i: (res[i]))


# In[257]:


classes=res[5:]


# In[258]:


classes


# In[259]:


import json


# In[260]:


data = json.load(open('download.json'))


# In[261]:


categories=data.keys()


# In[262]:


from collections import defaultdict

dd = defaultdict(list)


# In[263]:


for idx, item in enumerate(categories):
    dd[item].append(list(data[item].keys()))
dd


# In[264]:


subs=[]
for words in predictdata[0]:
    #print(words) 
    for values in classes:
        #print(values)
        for s in dd[values]:
            #print(s)
            if words in (s):
                subs.append(words)
                


# In[265]:


dd['sports']


# In[266]:


newres


# In[267]:


subs


# In[268]:


categories


# In[269]:


dd['politics']


# In[271]:


df.Derived_Category[1]
